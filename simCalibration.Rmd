---
title: "calerror"
output: html_document
date: '2024-01-29'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

compute_calibration_error <- function(f, Y) {
  #data <- generate_data_splits(10000, 2, 2, d = d)$data_train
  #X <- data$X
  #Y <- data$Y
  f <- as.vector(f)
  Y <- as.vector(Y)
  lrnr <-  make_learner(Pipeline, Lrnr_cv$new(Stack$new(
    Lrnr_xgboost$new(max_depth = 3),
    Lrnr_xgboost$new(max_depth = 4),
    Lrnr_xgboost$new(max_depth = 5),
    Lrnr_xgboost$new(max_depth = 6),
    Lrnr_xgboost$new(max_depth = 7),
    Lrnr_xgboost$new(max_depth = 8),
    Lrnr_xgboost$new(max_depth = 9)
  )), Lrnr_cv_selector$new(loss_squared_error))
  
  task <- sl3_Task$new(data.table(f,Y), covariates = "f", outcome = "Y")
  f_cal <- lrnr$train(task)$predict(task)
  # debiased calibration error estimator of https://proceedings.mlr.press/v151/xu22c/xu22c.pdf
  calibration_error <- sqrt(max(mean((f_cal - f)*(Y - f)), 0))
  return(calibration_error)
}


```


```{r}

################
#### Experiment 1a: effect of isotonic calibration on conformiy score efficiency
###############


library(ggplot2)
#set.seed(12345)
d <- 5
n_train <- 1000
n_test <- 1000
b = 0.6
alpha <- 0.1
library(sl3)
n_cal <- 1000
set.seed(12345)

get_widths_cal <- function(shape) {
  lrnr <- Lrnr_ranger$new()
  lrnr_name <- "random forests"
  # generate data splits
  data_list <- generate_data_splits(n_train, n_cal, n_test, d = d, distr_shift = TRUE, shape = shape, b = b)
  data_train <- data_list$data_train; data_cal <- data_list$data_cal; data_test <- data_list$data_test
  X_train <- data_train$X; X_cal <- data_cal$X; X_test <- data_test$X
  Y_train <- data_train$Y; Y_cal <- data_cal$Y; Y_test <- data_test$Y
  
  # train predictor
  predictor <- train_predictor(X_train, Y_train, lrnr)
  # get predictions
  f_cal <- predictor(X_cal)
   
  f_test <- predictor(X_test)
  
  # Do self-consistent conformal prediction
  out <- conformal_calibrator(f_train = f_cal, Y_train = Y_cal, f_test = f_test, calibrator = iso_calibrator, alpha = alpha, num_bins_Y = 100)
  
  # extract calibrated scores.
  scores_calibrated <- t(as.matrix(out$prediction_point(f_cal, return_score = TRUE, Y = Y_cal)))
  scores_calibrated <- apply(scores_calibrated, 1, max)
  scores_uncalibrated <- as.vector(abs(Y_cal - f_cal))
  width_cal <- quantile(scores_calibrated, ceiling(c(0.8, 0.9, 0.95) * (n_cal + 1))/n_cal)
  width_uncal <- quantile(scores_uncalibrated,ceiling(c(0.8, 0.9, 0.95) * (n_cal + 1))/n_cal)
  
  # get calibration error of initial predictor
  data_bench <- generate_data_splits(n_train = 2, n_cal = 10000, 2, d = d, distr_shift = TRUE, shape = shape, b = b)$data_cal
  X_bench <- data_bench$X
  Y_bench <- data_bench$Y
  f_bench <- predictor(X_bench)
  
  cal_error <- compute_calibration_error(f_bench, Y_bench)
  
  out <- data.table(n = n_cal, shape = shape, width = c(width_cal, width_uncal), alpha = rep(c(0.2, 0.1, 0.05), 2), Status = rep(c("Calibrated", "Uncalibrated"), each = 3), cal_error = cal_error)
  return(out)
}

results_all <- rbindlist(lapply(1:100, function(iter) {
  try({
  results <- rbindlist(lapply(c(1, 1.5, 2, 2.5, 3, 4, 5), get_widths_cal) )
  results$iter <- iter
  })
  return(results)
}))

results <- results_all[, .(width = mean(width), cal_error = mean(cal_error)), by = c("shape", "alpha", "Status")]


ggplot(results , aes(x = cal_error, y = width, color = as.factor(alpha), linetype = Status)) + geom_line()  

```




```{r}
library(ggplot2)
#set.seed(12345)
d <- 5
n_train <- 1000
n_test <- 1000
b = 0.6
alpha <- 0.1
library(sl3)
set.seed(12345)
shape <- 3


get_widths_cal <- function(n_cal) {
  lrnr <- Lrnr_ranger$new()
  lrnr_name <- "random forests"
  # generate data splits
  data_list <- generate_data_splits(n_train, n_cal, n_test, d = d, distr_shift = TRUE, shape = shape, b = b)
  data_train <- data_list$data_train; data_cal <- data_list$data_cal; data_test <- data_list$data_test
  X_train <- data_train$X; X_cal <- data_cal$X; X_test <- data_test$X
  Y_train <- data_train$Y; Y_cal <- data_cal$Y; Y_test <- data_test$Y
  
  # train predictor
  predictor <- train_predictor(X_train, Y_train, lrnr)
  # get predictions
  f_cal <- predictor(X_cal)
  
  f_test <- predictor(X_test)
  
  # Do self-consistent conformal prediction
  out <- conformal_calibrator(f_train = f_cal, Y_train = Y_cal, f_test = f_test, calibrator = iso_calibrator, alpha = alpha, num_bins_Y = 100)
  
  # extract calibrated scores.
  scores_calibrated <- t(as.matrix(out$prediction_point(f_cal, return_score = TRUE, Y = Y_cal)))
  scores_calibrated <- apply(scores_calibrated, 1, max)
  scores_uncalibrated <- as.vector(abs(Y_cal - f_cal))
  width_cal <- quantile(scores_calibrated, ceiling(c(0.8, 0.9, 0.95) * (n_cal + 1))/n_cal)
  width_uncal <- quantile(scores_uncalibrated,ceiling(c(0.8, 0.9, 0.95) * (n_cal + 1))/n_cal)
  
  # get calibration error of initial predictor
  data_bench <- generate_data_splits(n_train = 2, n_cal = 10000, 2, d = d, distr_shift = TRUE, shape = shape, b = b)$data_cal
  X_bench <- data_bench$X
  Y_bench <- data_bench$Y
  f_bench <- predictor(X_bench)
  
  cal_error <- compute_calibration_error(f_bench, Y_bench)
  
  out <- data.table(n = n_cal, shape = shape, width = c(width_cal, width_uncal), alpha = rep(c(0.2, 0.1, 0.05), 2), Status = rep(c("Calibrated", "Uncalibrated"), each = 3), cal_error = cal_error)
  return(out)
}

results_all <- rbindlist(lapply(1:010, function(iter) {
  try({
    print(iter)
  results <- rbindlist(lapply(c(20, 30, 40, 50, 75, 100, 200, 300 ), get_widths_cal) )
  results$iter <- iter
  })
  return(results)
}))

#, 30, 40, 50, 75, 100, 200, 300, 500
results <- results_all[, .(width = mean(width)), by = c("n", "alpha", "Status")]


ggplot(results  , aes(x = n, y = width, color = as.factor(alpha), linetype = Status)) + geom_smooth(se=F)  + scale_x_log10()




```
